# ğŸ¯ Dead by Daylight News API â€“ TODO List

A unified news aggregator for Dead by Daylight events, patch notes, codes, and media across all major platforms.

---

## Current Progress

- Express server with `/api/news` endpoint
- YouTube scraper (Prisma DB + upsert logic)
- Twitter scraper working and saving to DB
- Pagination, filtering by `source`, `type`
- Prisma database + schema implemented
- Nodemon for auto-reload in dev
- `.env` used for API keys (YouTube, Twitter, etc.)
- Prisma migrations tracked in Git

---

## Features by Area

### 1. Database: Unified Schema

**Model: `NewsItem`**

| Field | Type | Description |
|-------|------|-------------|
| id | String (UUID) | Primary key |
| title | String | Short title |
| content | String? | Description or body |
| url | String (unique) | Original link |
| imageUrl | String? | Thumbnail or preview |
| source | String | "YouTube", "Twitter", etc. |
| contentType | String | "video", "code", "update", etc. |
| code | String? | Promo/redeem code, if present |
| publishedAt | DateTime | Original publish timestamp |
| createdAt | DateTime | DB timestamp |

- Implemented via Prisma schema  
- Deduplication by `url` using `.upsert()`  
- Auto timestamps

---

### 2. Scrapers (One Module Per Source)

Each scraper returns an array of `NewsItem`-compatible objects.

| Source         | Status       | Tasks |
|----------------|--------------|-------|
| **YouTube**    | âœ… Working   | [ ] Regex for codes in video descriptions |
| **Twitter**    | âœ… Working   | [ ] Improve code parsing |
| **Instagram**  | ğŸ• Planned   | [ ] Puppeteer/cheerio scraper |
| **Official Site** | ğŸ• Planned | [ ] RSS or cheerio scraping |
| **Steam/Forums** | ğŸ• Planned | [ ] RSS or static scraper |
| **Reddit**     | ğŸ• Planned   | [ ] Pushshift or Reddit API |
| **Event Calendar Pages** | ğŸ• Bonus | [ ] Scrape upcoming events/timers |

All scrapers save to DB using the shared `saveData()` utility.

---

### 3. API Features

**Endpoint:** `/api/news`

| Feature | Status |
|--------|--------|
| GET all news | âœ… |
| Filter by `source` | âœ… |
| Filter by `type` | âœ… |
| Pagination (`page`, `limit`) | âœ… |
| Sort by newest first | âœ… |
| Add API docs | ğŸ”œ Planned |
| Add `/api/news/:id` | ğŸ”œ Optional |

---

### 4. Developer Experience

| Task | Status |
|------|--------|
| `.env` and dotenv config | âœ… |
| Prisma + SQLite local DB | âœ… |
| Modular folder structure | âœ… |
| Dev server with Nodemon | âœ… |
| Reusable `saveData()` for all scrapers | âœ… |
| Prisma client re-used via `/lib/prisma.js` | âœ… |

---

### 5. Infrastructure & Automation

| Feature | Status |
|---------|--------|
| Periodic scraping (cron or schedule) | ğŸ”œ Planned |
| GitHub Actions for lint/test/CI | ğŸ”œ Planned |
| Deploy API to Vercel / Fly.io / Render | ğŸ”œ Planned |
| Protect `.env` and secrets | âœ… |
| Add `prisma/dev.db` to `.gitignore` | âœ… |

---

### 6. Testing

| Task | Status |
|------|--------|
| Write scraper unit tests | ğŸ• Not started |
| Test database saving / deduplication | ğŸ• |
| Integration tests for `/api/news` | ğŸ• |
| Mocking external APIs | ğŸ• |

---

### 7. Docs

| Task | Status |
|------|--------|
| Update `README.md` with setup + usage | ğŸ”„ In Progress |
| Add example API responses | ğŸ• |
| Scraper dev guide | ğŸ• |
| Deployment instructions | ğŸ• |

---

## Next Actions (Suggestion)

1. Final cleanup: remove any unused `fs`, `data.json`, old logic
2. Finalize README.md with current usage
3. Test DB-saving from each scraper
4. Add Steam or Website scraper
5. Set up a scraping schedule (cron or interval)
6. Prepare for deployment

---

